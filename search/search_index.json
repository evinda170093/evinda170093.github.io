{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Material for Web Mining \u00b6 Nama : Evinda Widia Cahyaningrum NIM : 170411100093 Pembimbing : @mulaab Referensi : https://github.com/mulaab/webmining Github Source : https://github.com/evinda170093/evinda170093.github.io Crawling \u00b6 Web scraping sering disebut web crawling atau web spidering atau \"secara terprogram menelusuri kumpulan halaman web dan mengekstraksi data,\" adalah alat yang ampuh untuk bekerja dengan data di web. Read More Text Preprocessing \u00b6 Preprocessing atau Text Preprocessing merupakan tahap lanjutan dari crawling data. Text preprocessing merupakan tahap untuk melakukan seleksi data sebelum dilakukan pemrosesan data dan diubah menjadi data yang lebih tertsruktur. Read More Modelling \u00b6 Terdapat beberapa topik dalam modelling, diantaranya adalah LSA (Latent Semantic Analysis) dan LDA (Latent Dirichlet Allocation). LSA merupakah sebuah teknik dasar dalam pemodelan text untuk menganalisis hubungan antara Read More Evaluation \u00b6 Terdapat beberapa metrik evaluasi yang bisa digunakan untuk mengevaluasi hasil dari kerja sebuah sistem. Diantaranya adalah Recall, Precision, dan F1-Measure. Read More","title":"Summary of Web-Mining"},{"location":"#material_for_web_mining","text":"Nama : Evinda Widia Cahyaningrum NIM : 170411100093 Pembimbing : @mulaab Referensi : https://github.com/mulaab/webmining Github Source : https://github.com/evinda170093/evinda170093.github.io","title":"Material for Web Mining"},{"location":"#crawling","text":"Web scraping sering disebut web crawling atau web spidering atau \"secara terprogram menelusuri kumpulan halaman web dan mengekstraksi data,\" adalah alat yang ampuh untuk bekerja dengan data di web. Read More","title":"Crawling"},{"location":"#text_preprocessing","text":"Preprocessing atau Text Preprocessing merupakan tahap lanjutan dari crawling data. Text preprocessing merupakan tahap untuk melakukan seleksi data sebelum dilakukan pemrosesan data dan diubah menjadi data yang lebih tertsruktur. Read More","title":"Text Preprocessing"},{"location":"#modelling","text":"Terdapat beberapa topik dalam modelling, diantaranya adalah LSA (Latent Semantic Analysis) dan LDA (Latent Dirichlet Allocation). LSA merupakah sebuah teknik dasar dalam pemodelan text untuk menganalisis hubungan antara Read More","title":"Modelling"},{"location":"#evaluation","text":"Terdapat beberapa metrik evaluasi yang bisa digunakan untuk mengevaluasi hasil dari kerja sebuah sistem. Diantaranya adalah Recall, Precision, dan F1-Measure. Read More","title":"Evaluation"},{"location":"crawling/","text":"Apa itu Web Crawling? \u00b6 Web scraping sering disebut web crawling atau web spidering atau \"secara terprogram menelusuri kumpulan halaman web dan mengekstraksi data,\" adalah alat yang ampuh untuk bekerja dengan data di web. Namun web crawling dan web scraping adalah dua konsep yang berbeda tetapi terkait. Web crawling adalah komponen scraping web, logika crawling menemukan URL untuk diproses oleh kode scraping. Web crawling dimulai dengan daftar URL untuk dikunjungi, yang disebut seed. Untuk setiap URL, crawling menemukan tautan dalam HTML, memfilter tautan tersebut berdasarkan beberapa kriteria dan menambahkan tautan baru ke antrean. Semua HTML atau beberapa informasi spesifik diekstraksi untuk diproses oleh saluran yang berbeda. Dengan web scraping, kita dapat menambang data tentang sekumpulan produk, mendapatkan kumpulan teks atau data kuantitatif yang besar untuk dimainkan, mendapatkan data dari situs tanpa API resmi, atau hanya memuaskan rasa ingin tahu pribadi. Dalam tutorial ini, Anda akan belajar tentang dasar-dasar proses crawling saat menjelajahi kumpulan data Strategi web crawling \u00b6 Dalam praktiknya, web crawling hanya mengunjungi sebagian halaman bergantung pada crawler budget, yang dapat berupa jumlah halaman maksimum per domain, kedalaman (depth), atau waktu eksekusi (execution time). Situs web paling populer menyediakan file robots.txt untuk menunjukkan area situs web mana yang tidak boleh dirayapi oleh setiap agen pengguna. Kebalikan dari file robots adalah file sitemap.xml, yang berisi daftar halaman yang dapat di crawling. Kasus penggunaan web crawler yang populer meliputi: Search engines (Googlebot, Bingbot, Yandex Bot\u2026) mengumpulkan semua HTML untuk bagian penting dari Web. Data ini diindeks untuk membuatnya dapat dicari. Alat analisis SEO selain mengumpulkan HTML juga mengumpulkan metadata seperti waktu respons (response time), status respons (response status) untuk mendeteksi halaman yang rusak, dan tautan antara domain yang berbeda untuk mengumpulkan tautan balik. Alat pemantau harga crawl e-commerce websites untuk menemukan halaman produk dan mengekstrak metadata, terutama harganya. Halaman produk kemudian ditinjau kembali secara berkala. Common Crawl memelihara repositori terbuka dari data perayapan web . Misalnya, arsip dari Oktober 2020 berisi 2,71 miliar halaman web. Selanjutnya, kita akan membandingkan tiga strategi berbeda untuk membangun web crawler dengan Python. Pertama, hanya menggunakan library standar, lalu library pihak ketiga untuk membuat permintaan HTTP dan mem-parsing HTML, dan terakhir, kerangka kerja perayapan web Membangun web crawler sederhana dengan Python dari awal \u00b6 Untuk membangun web crawler sederhana dengan Python, kita memerlukan setidaknya satu library untuk mengunduh HTML dari URL dan library parsing HTML untuk mengekstrak links. Python menyediakan urllib library standar untuk membuat permintaan HTTP dan html.parser untuk mem -parsing HTML. Contoh crawler Python yang dibuat hanya dengan library standar dapat ditemukan di Github. Library Python standar untuk request dan patsing HTML tidak terlalu ramah pengembang. Libary populer lainnya seperti request , dicap sebagai HTTP untuk manusia, dan Beautiful Soup memberikan pengalaman pengembang yang lebih baik. Kita dapat menginstal dua library secara lokal. Disini kita akan melakukan crawling berita pada website CNN Indonesia pip install requests bs4 Crawler dasar dapat dibangun mengikuti diagram arsitektur sebelumnya. Library yang dibutuhkan \u00b6 import requests from bs4 import BeautifulSoup import numpy as np import pandas as pd import re import time Function get news \u00b6 Untuk mendapatkan data berita dari website yang akan kita crawling, buat sebuah function untuk melakukan web crawler. def get_news_cnn (): # url definition url = \"https://www.cnnindonesia.com/teknologi/indeks/8\" # Request r1 = requests . get ( url ) r1 . status_code # We'll save in coverpage the cover page content coverpage = r1 . content # Soup creation soup1 = BeautifulSoup ( coverpage , 'html5lib' ) # News identification coverpage_news = soup1 . find_all ( 'div' , class_ = 'list media_rows middle' ) for arti_i in coverpage_news : arti = arti_i . find_all ( 'article' ) #print(len(arti)) number_of_articles = 3 # Empty lists for content, links and titles news_contents = [] list_links = [] list_titles = [] for n in np . arange ( 0 , number_of_articles ): # Getting the link of the article link = arti [ n ] . find ( 'a' )[ 'href' ] list_links . append ( link ) # Getting the title title = arti [ n ] . find ( 'h2' ) . get_text () list_titles . append ( title ) # Reading the content (it is divided in paragraphs) article = requests . get ( link ) article_content = article . content soup_article = BeautifulSoup ( article_content , 'html5lib' ) body = soup_article . find_all ( 'div' , class_ = 'detail_text' ) # Unifying the paragraphs list_paragraphs = [] final_article = [] for pg in body : get_paragraph = pg . find_all ( 'p' ) #print(get_paragraph) #print(len(get_paragraph)) for p in np . arange ( 0 , len ( get_paragraph )): paragraph = get_paragraph [ p ] . get_text () clean_paragraph = clean ( paragraph ) list_paragraphs . append ( clean_paragraph ) final_article = \" \" . join ( list_paragraphs ) # Removing special characters final_article = re . sub ( \" \\\\ xa0\" , \"\" , final_article ) news_contents . append ( final_article ) Kode di atas mendefinisikan kelas Crawler dengan hepler method untuk mendownload_url menggunakan library request, get_linked_urls menggunakan library Beautiful Soup. Kita dapat menjalankan crawler di terminal. Salah satu metode HTTP yang paling umum adalah GET. The GETmetode menunjukkan bahwa Anda sedang berusaha untuk mendapatkan atau mengambil data dari sumber tertentu. Untuk membuat GETpermintaan, panggil requests.get(). Untuk mengujinya, kita dapat membuat GETrequest ke alamat website CNN dengan memanggil get() URL berikut: url = \"https://www.cnnindonesia.com/teknologi/indeks/8\" r1 = requests . get ( url ) Informasi pertama yang dapat kita kumpulkan adalah status code. Kode status memberi tahu kita tentang status permintaan. Misalnya, 200 OK status berarti permintaan Anda berhasil, sedangkan 404 NOT FOUNDstatus berarti sumber daya yang kita cari tidak ditemukan. Dengan mengakses .status_code, kita dapat melihat kode status yang dikembalikan server: r1 . status_code Untuk mendapatkan konten dari website kita perlu menyimpan body html yang kita crawling dengan .content : coverpage = r1 . content Hal yang sangat menyenangkan tentang library BeautifulSoup adalah ia dibangun di atas library parsing HTML seperti html5lib, lxml, html.parser, dll. Jadi objek BeautifulSoup dan menentukan library parser dapat dibuat pada saat yang bersamaan. soup1 = BeautifulSoup ( coverpage , 'html5lib' ) kita membuat objek Beautiful Soup dengan melewatkan dua argumen page.content (coverpage): Ini adalah konten html asli html5lib: Menentukan parser html yang ingin kita gunakan Untuk mencari seluruh tag < div > dengan class list media_rows middle dimana didalamnya terdapat beberapa kumpulan berita yang terdapat pada tag < article > dengan batasan untuk 3 berita saja, num_of_articles = 3 coverpage_news = soup1 . find_all ( 'div' , class_ = 'list media_rows middle' ) for arti_i in coverpage_news : arti = arti_i . find_all ( 'article' ) #print(len(arti)) number_of_articles = 3 Untuk mendapatkan seluruh paragraf atau kalimat yang terdapat pada berita maka digunakan code dibawah ini news_contents = [] list_links = [] list_titles = [] for n in np . arange ( 0 , number_of_articles ): link = arti [ n ] . find ( 'a' )[ 'href' ] list_links . append ( link ) title = arti [ n ] . find ( 'h2' ) . get_text () list_titles . append ( title ) article = requests . get ( link ) article_content = article . content soup_article = BeautifulSoup ( article_content , 'html5lib' ) body = soup_article . find_all ( 'div' , class_ = 'detail_text' ) list_paragraphs = [] final_article = [] for pg in body : get_paragraph = pg . find_all ( 'p' ) Dimana yang akan diambil adalah link, title dan isi berita, link diambil dari tag < a > dan title diambil dari tag < h2 > dan untuk isi berita diambil dari tag < p > Function Clean \u00b6 Code diatas menghasilkan text dengan tulisan \\n dan \\t yang dika diartikan adalah tab dan enter dimana hal tersebut dihilangkan dengan code berikut : def clean ( string ): string = string . replace ( \" \\n \" , \" \" ) string = string . replace ( \" \\t \" , \" \" ) string = string . strip () #string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string) string = string . strip () arrString = string . split ( \" \" ) return string . strip () Dan digunakan atau dipanggil pada code dibawah ini untuk menghilangkan tab dan enter pada setiap paragraf dari berita for p in np . arange ( 0 , len ( get_paragraph )): paragraph = get_paragraph [ p ] . get_text () clean_paragraph = clean ( paragraph ) list_paragraphs . append ( clean_paragraph ) final_article = \" \" . join ( list_paragraphs ) Terkadang text yang diambil dari hasil crawler tidak begitu bagus, masih terdapat banyak sekali simbol atau special character yang tidak dibutuhkan sehingga disini perlu adanya proses cleansing untuk menghilangkan special character yang mengganggu text. final_article = re . sub ( \" \\\\ xa0\" , \"\" , final_article ) news_contents . append ( final_article )","title":"Crawling"},{"location":"crawling/#apa_itu_web_crawling","text":"Web scraping sering disebut web crawling atau web spidering atau \"secara terprogram menelusuri kumpulan halaman web dan mengekstraksi data,\" adalah alat yang ampuh untuk bekerja dengan data di web. Namun web crawling dan web scraping adalah dua konsep yang berbeda tetapi terkait. Web crawling adalah komponen scraping web, logika crawling menemukan URL untuk diproses oleh kode scraping. Web crawling dimulai dengan daftar URL untuk dikunjungi, yang disebut seed. Untuk setiap URL, crawling menemukan tautan dalam HTML, memfilter tautan tersebut berdasarkan beberapa kriteria dan menambahkan tautan baru ke antrean. Semua HTML atau beberapa informasi spesifik diekstraksi untuk diproses oleh saluran yang berbeda. Dengan web scraping, kita dapat menambang data tentang sekumpulan produk, mendapatkan kumpulan teks atau data kuantitatif yang besar untuk dimainkan, mendapatkan data dari situs tanpa API resmi, atau hanya memuaskan rasa ingin tahu pribadi. Dalam tutorial ini, Anda akan belajar tentang dasar-dasar proses crawling saat menjelajahi kumpulan data","title":"Apa itu Web Crawling?"},{"location":"crawling/#strategi_web_crawling","text":"Dalam praktiknya, web crawling hanya mengunjungi sebagian halaman bergantung pada crawler budget, yang dapat berupa jumlah halaman maksimum per domain, kedalaman (depth), atau waktu eksekusi (execution time). Situs web paling populer menyediakan file robots.txt untuk menunjukkan area situs web mana yang tidak boleh dirayapi oleh setiap agen pengguna. Kebalikan dari file robots adalah file sitemap.xml, yang berisi daftar halaman yang dapat di crawling. Kasus penggunaan web crawler yang populer meliputi: Search engines (Googlebot, Bingbot, Yandex Bot\u2026) mengumpulkan semua HTML untuk bagian penting dari Web. Data ini diindeks untuk membuatnya dapat dicari. Alat analisis SEO selain mengumpulkan HTML juga mengumpulkan metadata seperti waktu respons (response time), status respons (response status) untuk mendeteksi halaman yang rusak, dan tautan antara domain yang berbeda untuk mengumpulkan tautan balik. Alat pemantau harga crawl e-commerce websites untuk menemukan halaman produk dan mengekstrak metadata, terutama harganya. Halaman produk kemudian ditinjau kembali secara berkala. Common Crawl memelihara repositori terbuka dari data perayapan web . Misalnya, arsip dari Oktober 2020 berisi 2,71 miliar halaman web. Selanjutnya, kita akan membandingkan tiga strategi berbeda untuk membangun web crawler dengan Python. Pertama, hanya menggunakan library standar, lalu library pihak ketiga untuk membuat permintaan HTTP dan mem-parsing HTML, dan terakhir, kerangka kerja perayapan web","title":"Strategi web crawling"},{"location":"crawling/#membangun_web_crawler_sederhana_dengan_python_dari_awal","text":"Untuk membangun web crawler sederhana dengan Python, kita memerlukan setidaknya satu library untuk mengunduh HTML dari URL dan library parsing HTML untuk mengekstrak links. Python menyediakan urllib library standar untuk membuat permintaan HTTP dan html.parser untuk mem -parsing HTML. Contoh crawler Python yang dibuat hanya dengan library standar dapat ditemukan di Github. Library Python standar untuk request dan patsing HTML tidak terlalu ramah pengembang. Libary populer lainnya seperti request , dicap sebagai HTTP untuk manusia, dan Beautiful Soup memberikan pengalaman pengembang yang lebih baik. Kita dapat menginstal dua library secara lokal. Disini kita akan melakukan crawling berita pada website CNN Indonesia pip install requests bs4 Crawler dasar dapat dibangun mengikuti diagram arsitektur sebelumnya.","title":"Membangun web crawler sederhana dengan Python dari awal"},{"location":"crawling/#library_yang_dibutuhkan","text":"import requests from bs4 import BeautifulSoup import numpy as np import pandas as pd import re import time","title":"Library yang dibutuhkan"},{"location":"crawling/#function_get_news","text":"Untuk mendapatkan data berita dari website yang akan kita crawling, buat sebuah function untuk melakukan web crawler. def get_news_cnn (): # url definition url = \"https://www.cnnindonesia.com/teknologi/indeks/8\" # Request r1 = requests . get ( url ) r1 . status_code # We'll save in coverpage the cover page content coverpage = r1 . content # Soup creation soup1 = BeautifulSoup ( coverpage , 'html5lib' ) # News identification coverpage_news = soup1 . find_all ( 'div' , class_ = 'list media_rows middle' ) for arti_i in coverpage_news : arti = arti_i . find_all ( 'article' ) #print(len(arti)) number_of_articles = 3 # Empty lists for content, links and titles news_contents = [] list_links = [] list_titles = [] for n in np . arange ( 0 , number_of_articles ): # Getting the link of the article link = arti [ n ] . find ( 'a' )[ 'href' ] list_links . append ( link ) # Getting the title title = arti [ n ] . find ( 'h2' ) . get_text () list_titles . append ( title ) # Reading the content (it is divided in paragraphs) article = requests . get ( link ) article_content = article . content soup_article = BeautifulSoup ( article_content , 'html5lib' ) body = soup_article . find_all ( 'div' , class_ = 'detail_text' ) # Unifying the paragraphs list_paragraphs = [] final_article = [] for pg in body : get_paragraph = pg . find_all ( 'p' ) #print(get_paragraph) #print(len(get_paragraph)) for p in np . arange ( 0 , len ( get_paragraph )): paragraph = get_paragraph [ p ] . get_text () clean_paragraph = clean ( paragraph ) list_paragraphs . append ( clean_paragraph ) final_article = \" \" . join ( list_paragraphs ) # Removing special characters final_article = re . sub ( \" \\\\ xa0\" , \"\" , final_article ) news_contents . append ( final_article ) Kode di atas mendefinisikan kelas Crawler dengan hepler method untuk mendownload_url menggunakan library request, get_linked_urls menggunakan library Beautiful Soup. Kita dapat menjalankan crawler di terminal. Salah satu metode HTTP yang paling umum adalah GET. The GETmetode menunjukkan bahwa Anda sedang berusaha untuk mendapatkan atau mengambil data dari sumber tertentu. Untuk membuat GETpermintaan, panggil requests.get(). Untuk mengujinya, kita dapat membuat GETrequest ke alamat website CNN dengan memanggil get() URL berikut: url = \"https://www.cnnindonesia.com/teknologi/indeks/8\" r1 = requests . get ( url ) Informasi pertama yang dapat kita kumpulkan adalah status code. Kode status memberi tahu kita tentang status permintaan. Misalnya, 200 OK status berarti permintaan Anda berhasil, sedangkan 404 NOT FOUNDstatus berarti sumber daya yang kita cari tidak ditemukan. Dengan mengakses .status_code, kita dapat melihat kode status yang dikembalikan server: r1 . status_code Untuk mendapatkan konten dari website kita perlu menyimpan body html yang kita crawling dengan .content : coverpage = r1 . content Hal yang sangat menyenangkan tentang library BeautifulSoup adalah ia dibangun di atas library parsing HTML seperti html5lib, lxml, html.parser, dll. Jadi objek BeautifulSoup dan menentukan library parser dapat dibuat pada saat yang bersamaan. soup1 = BeautifulSoup ( coverpage , 'html5lib' ) kita membuat objek Beautiful Soup dengan melewatkan dua argumen page.content (coverpage): Ini adalah konten html asli html5lib: Menentukan parser html yang ingin kita gunakan Untuk mencari seluruh tag < div > dengan class list media_rows middle dimana didalamnya terdapat beberapa kumpulan berita yang terdapat pada tag < article > dengan batasan untuk 3 berita saja, num_of_articles = 3 coverpage_news = soup1 . find_all ( 'div' , class_ = 'list media_rows middle' ) for arti_i in coverpage_news : arti = arti_i . find_all ( 'article' ) #print(len(arti)) number_of_articles = 3 Untuk mendapatkan seluruh paragraf atau kalimat yang terdapat pada berita maka digunakan code dibawah ini news_contents = [] list_links = [] list_titles = [] for n in np . arange ( 0 , number_of_articles ): link = arti [ n ] . find ( 'a' )[ 'href' ] list_links . append ( link ) title = arti [ n ] . find ( 'h2' ) . get_text () list_titles . append ( title ) article = requests . get ( link ) article_content = article . content soup_article = BeautifulSoup ( article_content , 'html5lib' ) body = soup_article . find_all ( 'div' , class_ = 'detail_text' ) list_paragraphs = [] final_article = [] for pg in body : get_paragraph = pg . find_all ( 'p' ) Dimana yang akan diambil adalah link, title dan isi berita, link diambil dari tag < a > dan title diambil dari tag < h2 > dan untuk isi berita diambil dari tag < p >","title":"Function get news"},{"location":"crawling/#function_clean","text":"Code diatas menghasilkan text dengan tulisan \\n dan \\t yang dika diartikan adalah tab dan enter dimana hal tersebut dihilangkan dengan code berikut : def clean ( string ): string = string . replace ( \" \\n \" , \" \" ) string = string . replace ( \" \\t \" , \" \" ) string = string . strip () #string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', string) string = string . strip () arrString = string . split ( \" \" ) return string . strip () Dan digunakan atau dipanggil pada code dibawah ini untuk menghilangkan tab dan enter pada setiap paragraf dari berita for p in np . arange ( 0 , len ( get_paragraph )): paragraph = get_paragraph [ p ] . get_text () clean_paragraph = clean ( paragraph ) list_paragraphs . append ( clean_paragraph ) final_article = \" \" . join ( list_paragraphs ) Terkadang text yang diambil dari hasil crawler tidak begitu bagus, masih terdapat banyak sekali simbol atau special character yang tidak dibutuhkan sehingga disini perlu adanya proses cleansing untuk menghilangkan special character yang mengganggu text. final_article = re . sub ( \" \\\\ xa0\" , \"\" , final_article ) news_contents . append ( final_article )","title":"Function Clean"},{"location":"evaluation/","text":"Apa itu Evaluation? \u00b6 Terdapat beberapa metrik evaluasi yang bisa digunakan untuk mengevaluasi hasil dari kerja sebuah sistem. Diantaranya adalah Recall, Precision, dan F1-Measure. Recall adalah rasio pengamatan positif yang diprediksi benar positif dengan seluruh data. Precision adalah rasio pengamatan positif yang diprediksi benar dengan total pengamatan positif yang diprediksi. F1-Measure adalah rata-rata tertimbang dari Recall dan Precision. Berikut adalah rumusnya : Presisi = TP/TP+FP Recall = TP/TP+FN F1-Measure = 2*(Recall * Precision) / (Recall + Precision) Dan berikut adalah code dari metode evaluasi Recall, Precision, dan F1-Measure : Library yang dibutuhkan \u00b6 from sklearn.metrics import classification_report , confusion_matrix , accuracy_score print ( confusion_matrix ( array_label , y_pred )) print ( classification_report ( array_label , y_pred )) print ( accuracy_score ( array_label , y_pred ))","title":"Evaluation"},{"location":"evaluation/#apa_itu_evaluation","text":"Terdapat beberapa metrik evaluasi yang bisa digunakan untuk mengevaluasi hasil dari kerja sebuah sistem. Diantaranya adalah Recall, Precision, dan F1-Measure. Recall adalah rasio pengamatan positif yang diprediksi benar positif dengan seluruh data. Precision adalah rasio pengamatan positif yang diprediksi benar dengan total pengamatan positif yang diprediksi. F1-Measure adalah rata-rata tertimbang dari Recall dan Precision. Berikut adalah rumusnya : Presisi = TP/TP+FP Recall = TP/TP+FN F1-Measure = 2*(Recall * Precision) / (Recall + Precision) Dan berikut adalah code dari metode evaluasi Recall, Precision, dan F1-Measure :","title":"Apa itu Evaluation?"},{"location":"evaluation/#library_yang_dibutuhkan","text":"from sklearn.metrics import classification_report , confusion_matrix , accuracy_score print ( confusion_matrix ( array_label , y_pred )) print ( classification_report ( array_label , y_pred )) print ( accuracy_score ( array_label , y_pred ))","title":"Library yang dibutuhkan"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"modelling/","text":"Apa itu Modelling? \u00b6 Terdapat beberapa topik dalam modelling, diantaranya adalah LSA (Latent Semantic Analysis) dan LDA (Latent Dirichlet Allocation). LSA merupakah sebuah teknik dasar dalam pemodelan text untuk menganalisis hubungan antara sekumpulan dokumen dan istilah atau term yang dikandungnya atau dapat didefinisikan sebagai model untuk mengekstraksi dan mewakili makna penggunaan kontekstual dari kata-kata dan untuk menghitung kesamaan antar kata, kalimat atau seluruh dokumen. LSA juga memanfaatkan konteks di sekitar kata untuk menangkap konsep tersembunyi yang juga dikenal sebagai topik menggunakan Singular Value Decomposition (SVD) Berikut adalah code mengenai LSA dan penjelasannya : Library yang dibutuhkan \u00b6 from scipy.linalg import svd from numpy import dot SDV (Singular Value Decomposition) adalah salah satu teknik dalam matrix factorization atau faktorisasi matriks yang banyak digunakan untuk menguraikan matriks menjadi beberapa matriks komponen. Dan berikut adalah code untuk SVD dengan menggunakan function svd yang terdapat pada module scipy.linalg : matrix_A = np . transpose ( tf ) print ( \"Ukuran Matrix A = \" , matrix_A . shape ) # SVD U , D , VT = svd ( matrix_A ) print ( \"Matrix U = \" , U . shape ) print ( \"Matrix S = \" , D . shape ) print ( \"Matrix Vt = \" , VT . shape ) # create m x n Sigma matrix Sigma = np . zeros (( matrix_A . shape [ 0 ], matrix_A . shape [ 1 ])) # populate Sigma with n x n diagonal matrix Sigma [: matrix_A . shape [ 1 ], : matrix_A . shape [ 1 ]] = np . diag ( D ) print ( \"Matrix Sigma = \" , Sigma . shape ) # reconstruct matrix B = U . dot ( Sigma . dot ( VT )) print ( \"Matrix A reconstruct = \" , B . shape )","title":"Modelling"},{"location":"modelling/#apa_itu_modelling","text":"Terdapat beberapa topik dalam modelling, diantaranya adalah LSA (Latent Semantic Analysis) dan LDA (Latent Dirichlet Allocation). LSA merupakah sebuah teknik dasar dalam pemodelan text untuk menganalisis hubungan antara sekumpulan dokumen dan istilah atau term yang dikandungnya atau dapat didefinisikan sebagai model untuk mengekstraksi dan mewakili makna penggunaan kontekstual dari kata-kata dan untuk menghitung kesamaan antar kata, kalimat atau seluruh dokumen. LSA juga memanfaatkan konteks di sekitar kata untuk menangkap konsep tersembunyi yang juga dikenal sebagai topik menggunakan Singular Value Decomposition (SVD) Berikut adalah code mengenai LSA dan penjelasannya :","title":"Apa itu Modelling?"},{"location":"modelling/#library_yang_dibutuhkan","text":"from scipy.linalg import svd from numpy import dot SDV (Singular Value Decomposition) adalah salah satu teknik dalam matrix factorization atau faktorisasi matriks yang banyak digunakan untuk menguraikan matriks menjadi beberapa matriks komponen. Dan berikut adalah code untuk SVD dengan menggunakan function svd yang terdapat pada module scipy.linalg : matrix_A = np . transpose ( tf ) print ( \"Ukuran Matrix A = \" , matrix_A . shape ) # SVD U , D , VT = svd ( matrix_A ) print ( \"Matrix U = \" , U . shape ) print ( \"Matrix S = \" , D . shape ) print ( \"Matrix Vt = \" , VT . shape ) # create m x n Sigma matrix Sigma = np . zeros (( matrix_A . shape [ 0 ], matrix_A . shape [ 1 ])) # populate Sigma with n x n diagonal matrix Sigma [: matrix_A . shape [ 1 ], : matrix_A . shape [ 1 ]] = np . diag ( D ) print ( \"Matrix Sigma = \" , Sigma . shape ) # reconstruct matrix B = U . dot ( Sigma . dot ( VT )) print ( \"Matrix A reconstruct = \" , B . shape )","title":"Library yang dibutuhkan"},{"location":"text-preprocessing/","text":"Apa itu Preprocessing? \u00b6 Preprocessing atau Text Preprocessing merupakan tahap lanjutan dari crawling data. Text preprocessing merupakan tahap untuk melakukan seleksi data sebelum dilakukan pemrosesan data dan diubah menjadi data yang lebih tertsruktur. Terdapat beberapa langkah dalam Text Preprocessing : 1. Tokenizing 2. Remove Stop Word 3. Stemming 4. Matrix TF-IDF Library yang dibutuhkan \u00b6 import numpy as np import sys from IPython.display import Image import matplotlib.pyplot as plt % matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # create stemmer import re # impor modul regular expressionkalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize import pandas as pd from sklearn.feature_extraction.text import TfidfTransformer Tokenizing \u00b6 Proses tokenizing atau pemisahan string merupakan proses untuk mengubah teks menjadi daftar token. Kita dapat menganggap token sebagai bagian seperti kata adalah tanda dalam kalimat dan kalimat adalah tanda dalam paragraf. Atau lebih mudah dipahami bahwa tokenizing dalah memisahkan kalimat dalam paragraf. Dalam contohnya tokenizing penggunakan function PunktSentenceTokenizer() dari module nltk.tokenize.punkt untuk menandai akhir dan awal kalimat, apakah terdapat karakter dan tanda baca. re merupakan module regular expressions untuk menghapus substring pada string. (r\"\\d+\", \"\",i) berarti merubah \"\\d+\" menjadi string kosong. Kemudian pada perulangan kedua menghapus enter dan menggantinya dengan string kosong untuk menggabungkan setiap kalimat yang ada. # TOKENIZING doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( document ) dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen ) Remove Stop Words \u00b6 Remove stop words merupakah salah satu langkah yang paling penting dalam text preprocessing. Dalam remove stop words terdapat proses untuk menyaring dan menghilangkan data atau kata yang tidak berguna. Dan dalam Natural Language Processing (NLP) kata-kata atau data yang tidak berguna tersebut dikenal dengan stopwords. Dalam stopwords terdiri dari kata-kata seperti kata penghubung (dan, atau, dari, di, dsb). Untuk proses remove stopwords menggunakan function StopWordRemoverFactory() dari module Sastrawi.StopWordRemover.StopWordRemoverFactory. Pertama kita membuat perulangan untuk menghapus kata-kata yang tidak penting pada setiap kalimat. Setalah dilakukan proses remove stopwords kita perlu memastikan bahwa setiap kata tidak berisikan tanda baca dengan menggunakan translate(str.maketrans(\"\",\"\",string.punctuation)) factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) dokumenstopall = [] for i in dokumenstop : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstopall . append ( output ) print ( \"Final \" , dokumenstopall ) Stemming \u00b6 Proses stemming adalah proses untuk menghasilkan kata dasar dari sebuah kalimat yang terdapat imbuhan. Contohnya : mengambil menjadi ambil, diproses menjadi proses dan sebagainya. Proses stemming menggunakan function StemmerFactory() dari module Sastrawi.Stemmer.StemmerFactory # STEMMING factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstopall : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem ) Get TF-IDF \u00b6 TF-IDF (Term Frequency-Inverse Document Frequency) adalah ukuran statistik yang mengvaluasi seberapa relevan sebuah kata dengan dokumen dalam kumpulan dokumen. TF-IDF diciptakan untuk pencarian dokumen dan temu kembali informasi. TF-IDF dihitung dengan mengalikan dua metrik yang berbeda : 1. Term Frequency (TF) : adalah frekuensi sebuah kata dalam sebuah dokumen. Cara paling sederhana adalah dengan menghitung jumlah awal kata yang muncul dalam dokumen. 2. Inverse Document Frequency (IDF) : adalah frekuensi kata di satu set dokumen. Yang berarti seberapa umum atau jarang sebuah kata di seluruh kumpulan dokumen. Semakin dekat ke 0, semakin umum sebuah kata. Metrik ini dapat dihitung dengan mengambil jumlah total dokumen, membaginya dengan jumlah dokumen yang berisi kata dan menghitung logaritma. Jika kata tersebut sangat umum dan muncul di banyak dokumen, maka hasil akan mendekati 0, jika tidak maka akan mendekati 1. Dan berikut adalah code untuk TF-IDF : count = CountVectorizer () bag = count . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () a = count . get_feature_names () dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ]) dfb tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( count . fit_transform ( dokumenstem )) . toarray () print ( tf . shape ) get_tfidf = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ]) get_tfidf","title":"Text Preprocessing"},{"location":"text-preprocessing/#apa_itu_preprocessing","text":"Preprocessing atau Text Preprocessing merupakan tahap lanjutan dari crawling data. Text preprocessing merupakan tahap untuk melakukan seleksi data sebelum dilakukan pemrosesan data dan diubah menjadi data yang lebih tertsruktur. Terdapat beberapa langkah dalam Text Preprocessing : 1. Tokenizing 2. Remove Stop Word 3. Stemming 4. Matrix TF-IDF","title":"Apa itu Preprocessing?"},{"location":"text-preprocessing/#library_yang_dibutuhkan","text":"import numpy as np import sys from IPython.display import Image import matplotlib.pyplot as plt % matplotlib inline import networkx as nx from nltk.tokenize.punkt import PunktSentenceTokenizer from sklearn.feature_extraction.text import TfidfTransformer , CountVectorizer import string from Sastrawi.Stemmer.StemmerFactory import StemmerFactory # create stemmer import re # impor modul regular expressionkalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\" from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from nltk.tokenize import word_tokenize import pandas as pd from sklearn.feature_extraction.text import TfidfTransformer","title":"Library yang dibutuhkan"},{"location":"text-preprocessing/#tokenizing","text":"Proses tokenizing atau pemisahan string merupakan proses untuk mengubah teks menjadi daftar token. Kita dapat menganggap token sebagai bagian seperti kata adalah tanda dalam kalimat dan kalimat adalah tanda dalam paragraf. Atau lebih mudah dipahami bahwa tokenizing dalah memisahkan kalimat dalam paragraf. Dalam contohnya tokenizing penggunakan function PunktSentenceTokenizer() dari module nltk.tokenize.punkt untuk menandai akhir dan awal kalimat, apakah terdapat karakter dan tanda baca. re merupakan module regular expressions untuk menghapus substring pada string. (r\"\\d+\", \"\",i) berarti merubah \"\\d+\" menjadi string kosong. Kemudian pada perulangan kedua menghapus enter dan menggantinya dengan string kosong untuk menggabungkan setiap kalimat yang ada. # TOKENIZING doc_tokenizer = PunktSentenceTokenizer () sentences_list = doc_tokenizer . tokenize ( document ) dokumenre = [] for i in sentences_list : hasil = re . sub ( r \"\\d+\" , \"\" , i ) dokumenre . append ( hasil ) dokumen = [] for i in dokumenre : hasil = i . replace ( ' \\n ' , '' ) dokumen . append ( hasil ) print ( dokumen )","title":"Tokenizing"},{"location":"text-preprocessing/#remove_stop_words","text":"Remove stop words merupakah salah satu langkah yang paling penting dalam text preprocessing. Dalam remove stop words terdapat proses untuk menyaring dan menghilangkan data atau kata yang tidak berguna. Dan dalam Natural Language Processing (NLP) kata-kata atau data yang tidak berguna tersebut dikenal dengan stopwords. Dalam stopwords terdiri dari kata-kata seperti kata penghubung (dan, atau, dari, di, dsb). Untuk proses remove stopwords menggunakan function StopWordRemoverFactory() dari module Sastrawi.StopWordRemover.StopWordRemoverFactory. Pertama kita membuat perulangan untuk menghapus kata-kata yang tidak penting pada setiap kalimat. Setalah dilakukan proses remove stopwords kita perlu memastikan bahwa setiap kata tidak berisikan tanda baca dengan menggunakan translate(str.maketrans(\"\",\"\",string.punctuation)) factory = StopWordRemoverFactory () stopword = factory . create_stop_word_remover () a = len ( dokumen ) dokumenstop = [] for i in range ( 0 , a ): sentence = stopword . remove ( dokumen [ i ]) dokumenstop . append ( sentence ) print ( dokumenstop ) dokumenstopall = [] for i in dokumenstop : output = i . translate ( str . maketrans ( \"\" , \"\" , string . punctuation )) dokumenstopall . append ( output ) print ( \"Final \" , dokumenstopall )","title":"Remove Stop Words"},{"location":"text-preprocessing/#stemming","text":"Proses stemming adalah proses untuk menghasilkan kata dasar dari sebuah kalimat yang terdapat imbuhan. Contohnya : mengambil menjadi ambil, diproses menjadi proses dan sebagainya. Proses stemming menggunakan function StemmerFactory() dari module Sastrawi.Stemmer.StemmerFactory # STEMMING factory = StemmerFactory () stemmer = factory . create_stemmer () dokumenstem = [] for i in dokumenstopall : output = stemmer . stem ( i ) dokumenstem . append ( output ) print ( dokumenstem )","title":"Stemming"},{"location":"text-preprocessing/#get_tf-idf","text":"TF-IDF (Term Frequency-Inverse Document Frequency) adalah ukuran statistik yang mengvaluasi seberapa relevan sebuah kata dengan dokumen dalam kumpulan dokumen. TF-IDF diciptakan untuk pencarian dokumen dan temu kembali informasi. TF-IDF dihitung dengan mengalikan dua metrik yang berbeda : 1. Term Frequency (TF) : adalah frekuensi sebuah kata dalam sebuah dokumen. Cara paling sederhana adalah dengan menghitung jumlah awal kata yang muncul dalam dokumen. 2. Inverse Document Frequency (IDF) : adalah frekuensi kata di satu set dokumen. Yang berarti seberapa umum atau jarang sebuah kata di seluruh kumpulan dokumen. Semakin dekat ke 0, semakin umum sebuah kata. Metrik ini dapat dihitung dengan mengambil jumlah total dokumen, membaginya dengan jumlah dokumen yang berisi kata dan menghitung logaritma. Jika kata tersebut sangat umum dan muncul di banyak dokumen, maka hasil akan mendekati 0, jika tidak maka akan mendekati 1. Dan berikut adalah code untuk TF-IDF : count = CountVectorizer () bag = count . fit_transform ( dokumenstem ) matrik_vsm = bag . toarray () a = count . get_feature_names () dfb = pd . DataFrame ( data = matrik_vsm , index = list ( range ( 1 , len ( matrik_vsm [:, 1 ]) + 1 , )), columns = [ a ]) dfb tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tf = tfidf . fit_transform ( count . fit_transform ( dokumenstem )) . toarray () print ( tf . shape ) get_tfidf = pd . DataFrame ( data = tf , index = list ( range ( 1 , len ( tf [:, 1 ]) + 1 , )), columns = [ a ]) get_tfidf","title":"Get TF-IDF"}]}